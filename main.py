import os
import streamlit as st
from dotenv import load_dotenv
import json
import requests
import openai
from openai import OpenAI
from openai import OpenAIError  # ä¿®æ­£
from openai_function_calling import Function, Parameter, JsonSchemaType
from openai_function_calling.tool_helpers import ToolHelpers
import logging
import pdfplumber
import random
from upstash_vector import Index

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Retrievalç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
RETRIEVAL_DIR = "retrieval_data"

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    logger.error("OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    st.error("APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

client = OpenAI(api_key=api_key)

# Streamlit ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="Advanced Chat with Miosync", page_icon="ğŸ’¬", layout="wide")

# ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜
st.title("ğŸ’¬ Advanced Chat with Miosync")
st.write("Chat with an AI Partner. You can customize the model and system behavior.")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []  # ä¼šè©±ãƒ­ã‚°ã‚’ç©ºãƒªã‚¹ãƒˆã§åˆæœŸåŒ–

if "selected_model" not in st.session_state:
    st.session_state.selected_model = "gpt-4o-2024-08-06"

if "temperature" not in st.session_state:
    st.session_state.temperature = 0.7

if "top_p" not in st.session_state:
    st.session_state.top_p = 1.0

if "max_tokens" not in st.session_state:
    st.session_state.max_tokens = 8000

if "system_prompt" not in st.session_state:  # ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡å­—åˆ—ã¨ã—ã¦åˆæœŸåŒ–
    st.session_state.system_prompt = "You are a helpful assistant. Use tools intelligently."

# å±¥æ­´ã®é•·ã•åˆ¶é™
MAX_HISTORY_LENGTH = 15

def trim_message_history():
    """
    å±¥æ­´ãŒé•·ã™ãã‚‹å ´åˆã€å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤
    """
    if len(st.session_state.messages) > MAX_HISTORY_LENGTH:
        st.session_state.messages = st.session_state.messages[-MAX_HISTORY_LENGTH:]

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã® Message Placeholder
message_placeholder = st.empty()

def update_chat_history():
    """
    ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ Message Placeholder ã«è¡¨ç¤º
    """
    with message_placeholder.container():
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                st.markdown(f"<div class='User-message' style='margin-bottom: 40px;'><b></b> {msg['content']}</div>", unsafe_allow_html=True)
            elif msg["role"] == "assistant":
                formatted_message = f"<b></b> {msg['content']}"
                st.markdown(
                    f"""
                    <div class="Partner-message" style="margin-bottom: 40px; padding: 20px; border-radius: 10px; background-color: #303030;">
                        <div style="display: flex; align-items: center; gap: 15px;">
                            <img src="https://www.miosync.link/img/list-kuromi.png" 
                                width="50" height="50" 
                                style="border-radius: 50%;" />
                            <div style="flex-grow: 1; word-wrap: break-word; font-size: 16px; line-height: 1.6;">
                                {formatted_message}
                            </div>
                        </div>
                    </div>
                    """, 
                    unsafe_allow_html=True
                )

def generate_embedding(data: list[str], model: str = "text-embedding-ada-002") -> list[list[float]]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼ã«å¤‰æ›ã™ã‚‹é–¢æ•°

    Args:
        data (list[str]): ãƒ™ã‚¯ã‚¿ãƒ¼åŒ–ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã€‚
        model (str): ä½¿ç”¨ã™ã‚‹Embeddingãƒ¢ãƒ‡ãƒ«ã€‚

    Returns:
        list[list[float]]: å„ãƒ†ã‚­ã‚¹ãƒˆã®Embeddingãƒ™ã‚¯ã‚¿ãƒ¼ã®ãƒªã‚¹ãƒˆã€‚
    """
    try:
        if not data:
            logger.warning("generate_embeddingã«ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆãŒæ¸¡ã•ã‚Œã¾ã—ãŸã€‚")
            return []

        logger.debug(f"Generating embeddings for data: {data}")
        openai_result = client.embeddings.create(input=data, model=model)  # 'client' ã‚’ä½¿ç”¨
        logger.debug(f"OpenAI Embedding API Response: {openai_result}")

        result_vectors = []

        # OpenAIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼é…åˆ—ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        for vector_response in openai_result.data:  # 'data' å±æ€§ã«ã‚¢ã‚¯ã‚»ã‚¹
            embedding = vector_response.embedding  # 'embedding' å±æ€§ã«ã‚¢ã‚¯ã‚»ã‚¹
            if embedding:
                result_vectors.append(embedding)
            else:
                logger.warning(f"EmbeddingãŒå­˜åœ¨ã—ãªã„ãƒ‡ãƒ¼ã‚¿: {vector_response}")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        logger.debug(f"Generated embeddings: {result_vectors}")

        logger.info(f"Generated {len(result_vectors)} embeddings.")
        return result_vectors
    except openai.error.OpenAIError as e:  # OpenAIErrorã‚’ä¿®æ­£
        logger.exception(f"OpenAI API ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    except Exception as e:
        logger.exception(f"ãƒ™ã‚¯ã‚¿ãƒ¼ç”Ÿæˆä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []


def search_documents(query: str, context: str = "", selected_category: str = "") -> dict:
    """
    ã‚¯ã‚¨ãƒªã‚’ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã«å¤‰æ›ã—ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œã—ã€
    çµæœã«åŸºã¥ã„ã¦PDFã‚’å‡¦ç†ã—ã¦å†…å®¹ã‚’å–å¾—ã€‚

    Args:
        query (str): æ¤œç´¢ã‚¯ã‚¨ãƒªã€‚
        context (str): ã‚¯ã‚¨ãƒªã®æ–‡è„ˆæƒ…å ±ã€‚
        selected_category (str): ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ã€‚

    Returns:
        dict: æ¤œç´¢çµæœã¨PDFå†…å®¹ã‚’å«ã‚€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚
    """
    # ã‚¯ã‚¨ãƒªã‚’ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã«å¤‰æ›
    embedding_list = generate_embedding([query])
    if not embedding_list:
        logger.error("ãƒ™ã‚¯ã‚¿ãƒ¼ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        return {"error": "ãƒ™ã‚¯ã‚¿ãƒ¼ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"}
    
    query_vector = embedding_list[0]
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰URLã¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
    endpoint = os.getenv("UPSTASH_VECTOR_ENDPOINT")
    token = os.getenv("UPSTASH_VECTOR_TOKEN")
    
    if not endpoint or not token:
        logger.error("UPSTASH_VECTOR_ENDPOINT ã¾ãŸã¯ UPSTASH_VECTOR_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return {"error": "æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã®è¨­å®šãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚"}
    
    index = Index(url=endpoint, token=token)
    
    try:
        # ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œ
        query_result = index.query(
            vector=query_vector,
            include_metadata=True,
            include_data=True,
            include_vectors=False,
            top_k=3,
        )

        # æ¤œç´¢çµæœã®æ•´å½¢
        formatted_results = [
            {
                "id": result.id,
                "score": result.score,
                "metadata": result.metadata,
                "data": result.data
            }
            for result in query_result
        ]
        logger.info(f"æ¤œç´¢çµæœ: {len(formatted_results)} ä»¶å–å¾—")

        # PDFå‡¦ç†ã®å®Ÿè¡Œ
        try:
            pdf_results = process_pdf_results(RETRIEVAL_DIR, formatted_results)

            # PDFå†…å®¹ã‚’æ¤œç´¢çµæœã«è¿½åŠ 
            for result in formatted_results:
                filename = result["metadata"].get("filename")
                if filename and filename in pdf_results and pdf_results[filename]:
                    result["pdf_content"] = pdf_results[filename][:500]  # æœ€åˆã®500æ–‡å­—ã‚’è¿½åŠ 
                else:
                    result["pdf_content"] = "é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

        except FileNotFoundError as e:
            logger.error(f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"PDFå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"}

        return {"results": formatted_results}

    except Exception as e:
        logger.exception("Upstash Vectorã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
        return {"error": f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"}


def process_pdf_results(retrieval_dir, formatted_results):
    """
    ã‚¯ã‚¨ãƒªçµæœã«åŸºã¥ã„ã¦PDFã‚’å‡¦ç†ã—ã€å†…å®¹ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    Args:
        retrieval_dir (str): PDFãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚
        formatted_results (list[dict]): æ•´å½¢ã•ã‚ŒãŸã‚¯ã‚¨ãƒªçµæœã€‚

    Returns:
        dict: ãƒ•ã‚¡ã‚¤ãƒ«åã¨æŠ½å‡ºã•ã‚ŒãŸå†…å®¹ã®è¾æ›¸ã€‚
    """
    if not os.path.exists(retrieval_dir):
        raise FileNotFoundError(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {retrieval_dir}")

    processed_results = {}
    for result in formatted_results:
        filename = result["metadata"].get("filename")
        if not filename:
            continue  # ãƒ•ã‚¡ã‚¤ãƒ«åãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

        file_path = os.path.join(retrieval_dir, filename)

        if os.path.exists(file_path):
            # PDFã‚’é–‹ã„ã¦å†…å®¹ã‚’æŠ½å‡º
            with pdfplumber.open(file_path) as pdf:
                text = ""
                for page in pdf.pages:
                    text += page.extract_text()
            processed_results[filename] = text
        else:
            processed_results[filename] = None  # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ

    return processed_results
                
# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
st.sidebar.title("Settings")
st.sidebar.subheader("Model Configuration")
st.session_state.selected_model = st.sidebar.selectbox(
    "Choose a model:",
     ["gpt-4o-2024-08-06", "gpt-4o-mini", "chatgpt-4o-latest"],
    index=["gpt-4o-2024-08-06", "gpt-4o-mini", "chatgpt-4o-latest"].index(st.session_state.selected_model)
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§System Promptã‚’è¨­å®š
st.sidebar.subheader("System Prompt")
st.session_state.system_prompt = st.sidebar.text_area(
    "Set the system's behavior prompt:",
    st.session_state.system_prompt
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§æ¸©åº¦ã€Top-pã€Max tokens ã‚’èª¿æ•´
st.sidebar.subheader("Generation Parameters")
st.session_state.temperature = st.sidebar.slider(
    "Temperature (Creativity)", 
    min_value=0.0, 
    max_value=1.0, 
    value=st.session_state.temperature, 
    step=0.01
)
st.session_state.top_p = st.sidebar.slider(
    "Top-p (Sampling)", 
    min_value=0.0, 
    max_value=1.0, 
    value=st.session_state.top_p, 
    step=0.01
)
st.session_state.max_tokens = st.sidebar.slider(
    "Max Tokens (Response Length)", 
    min_value=100, 
    max_value=12096, 
    value=st.session_state.max_tokens, 
    step=100
)

# search_documentsã‚’OpenAI Functionã¨ã—ã¦å®šç¾©
search_documents_function = Function(
    name="search_documents",
    description="Search for relevant documents using Upstash Vector DB.",
    parameters=[
        Parameter(
            name="query",
            type=JsonSchemaType.STRING,
            description="The search query."
        ),
        Parameter(
            name="context",
            type=JsonSchemaType.STRING,
            description="Context for the query."
        ),
        Parameter(
            name="selected_category",
            type=JsonSchemaType.STRING,
            description="The category to filter documents."
        )
    ]
)

tools = ToolHelpers.from_functions([search_documents_function])

def generate_response(user_input):
    """
    OpenAI APIã‚’ä½¿ç”¨ã—ã¦å¿œç­”ã‚’ç”Ÿæˆ
    ãƒ¢ãƒ‡ãƒ«ãŒfunction callingã™ã‚Œã°ãã‚Œã‚’å®Ÿè¡Œã€ãã®çµæœã‚’å†åº¦ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€æœ€çµ‚å›ç­”ã‚’å–å¾—
    """
    try:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": user_input})
        trim_message_history()
        update_chat_history()

        response = client.chat.completions.create(
            model=st.session_state.selected_model,
            messages=[
                {"role": "system", "content": st.session_state.system_prompt}  # å‹•çš„ã«è¿½åŠ 
            ] + st.session_state.messages,  # ä¼šè©±å±¥æ­´ã‚’è¿½åŠ 
            temperature=st.session_state.temperature,
            top_p=st.session_state.top_p,
            max_tokens=st.session_state.max_tokens,
            tools=tools,  # ãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ 
            tool_choice="auto"  # è‡ªå‹•é¸æŠ
        )

        response_message = response.choices[0].message

        # tool_callãŒç™ºç”Ÿã—ãŸå ´åˆ
        if hasattr(response_message, 'tool_calls') and response_message.tool_calls:
            tool_call = response_message.tool_calls[0]
            function_name = tool_call.function.name
            arguments = json.loads(tool_call.function.arguments)
            logger.info(f"Function Called: {function_name}")
            logger.info(f"Arguments: {arguments}")

            if function_name == "search_documents":
                query = arguments.get("query", "")

                # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
                function_response = search_documents(query)

                if "error" in function_response:
                    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
                    error_message = function_response["error"]
                    st.error(error_message)
                else:
                    # æ¤œç´¢çµæœã‚’æ•´å½¢ã—ã¦PDFå‡¦ç†ã‚’å®Ÿè¡Œ
                    formatted_results = [
                        {
                            "id": result["id"],
                            "score": result["score"],
                            "metadata": result["metadata"],
                            "data": result.get("data")
                        }
                        for result in function_response.get("results", [])
                    ]

                    try:
                        # PDFå‡¦ç†ã‚’å®Ÿè¡Œ
                        pdf_results = process_pdf_results(RETRIEVAL_DIR, formatted_results)

                        # PDFå†…å®¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ±åˆ
                        pdf_summary = []
                        for filename, content in pdf_results.items():
                            if content:
                                pdf_summary.append(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}\nå†…å®¹:\n{content[:500]}...\n")
                            else:
                                pdf_summary.append(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}\nå†…å®¹: é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n")

                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«PDFçµæœã‚’çµ±åˆ
                        prompt_context = "\n".join(pdf_summary)

                        # ãƒ¢ãƒ‡ãƒ«ã¸ã®å†å•ã„åˆã‚ã›
                        final_response = client.chat.completions.create(
                            model=st.session_state.selected_model,
                            messages=[
                                {"role": "system", "content": st.session_state.system_prompt},  # å‹•çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                                {"role": "user", "content": user_input},
                                {"role": "assistant", "content": prompt_context},  # PDFçµæœã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµ±åˆ
                            ],
                            temperature=st.session_state.temperature,
                            top_p=st.session_state.top_p,
                            max_tokens=st.session_state.max_tokens
                        )

                        # ãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚å¿œç­”ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
                        final_msg = final_response.choices[0].message
                        if hasattr(final_msg, 'content') and final_msg.content:
                            st.session_state.messages.append({"role": "assistant", "content": final_msg.content})
                        else:
                            logger.error("Final message content is missing.")
                    
                    except Exception as e:
                        logger.exception("Error during PDF processing.")
                        st.error(f"An error occurred during PDF processing: {e}")
        else:
            # function callãªã—ã§ç›´æ¥å›ç­”å–å¾—
            if hasattr(response_message, 'content') and response_message.content:
                st.session_state.messages.append({"role": "assistant", "content": response_message.content})

        update_chat_history()

    except Exception as e:
        logger.exception("Error during API call.")
        st.error(f"An error occurred: {e}")

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ (ä¸‹éƒ¨)
with st.form(key="chat_form"):
    user_input = st.text_input("Your message:", "")
    submit_button = st.form_submit_button("Send")

if submit_button:
    if user_input and user_input.strip():
        generate_response(user_input)
    else:
        st.error("Please enter a valid message.")

if st.button("Reset Chat"):
    # å±¥æ­´ã‚’å®Œå…¨ã«ç©ºãƒªã‚¹ãƒˆã§åˆæœŸåŒ–
    st.session_state.messages = []  
    st.success("Chat history reset.")
    update_chat_history()
else:
    update_chat_history()